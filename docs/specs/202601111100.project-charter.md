# Project Charter: Agnx

## Vision

> **Agnx is the "nginx for AI agents"** — a minimal, fast, self-hostable runtime that runs agents defined in a **transparent, portable format**, exposed through a standard API.

Agnx treats agents as durable artifacts: files you own that should outlast the runtime.

- **Transparent agent format** (human-readable, inspectable, versionable)
- **Stateless by default** (no hidden server-side state)
- **File-based state** when present (specs, memories, logs, config) — if Agnx disappears, take these and host elsewhere

## Problem Statement

Developers building AI-powered applications want a way to run agents where the **agent and its state are durable, portable artifacts** (not trapped inside a runtime):

1. A transparent, versionable agent definition
2. A stateless-by-default runtime with file-based, exportable state (specs, memories, logs, config)
3. A standard API, with provider-agnostic LLM integration

Current options require either:

- Buying into a heavy Python framework (LangGraph, CrewAI, Agno)
- Building everything from scratch
- Using proprietary, cloud-only services
- Accepting vendor lock-in for agent definitions

The ecosystem is fragmented, so Agnx proposes a practical standard today and will adopt any widely accepted, open agent definition standard once it emerges.

## Goals

### Primary Goals

1. **Lightweight & self-hostable** — Single binary, <5MB, starts in milliseconds
2. **Portable agent format** — YAML + Markdown agent definitions that work across runtimes
3. **Pluggable state & memory** — Defaults are filesystem; optional external backends (e.g. Postgres, Redis, S3)
4. **File-first state** — Agent specs + state are files you can copy, back up, and version
5. **Standards-compliant** — Agent Protocol API, MCP for tools, A2A for discovery (and follow widely adopted open standards as they emerge)
6. **LLM-agnostic** — OpenRouter, OpenAI, Anthropic, Ollama, any OpenAI-compatible API

### Secondary Goal: Edge & Embedded Capability

Agnx should run on resource-constrained devices (Raspberry Pi, edge servers, IoT gateways) without modification:

- **Single binary** — No runtime dependencies, no Python, no Node.js
- **Low memory footprint** — Target <50MB runtime memory
- **ARM-native** — First-class support for ARM64 (aarch64) and ARM32
- **Offline-capable** — Works with local LLMs without internet
- **Crash-resilient** — Stateless design handles power loss gracefully

## Non-Goals (v1.0)

- Web UI or visual builder
- Multi-agent orchestration
- Built-in RAG/vector search
- Workflow/DAG execution engine
- User authentication
- Requiring external infrastructure (DB, queues, etc.) for basic usage
- Proprietary agent formats or hidden state that can't be exported
- Competing with llama.cpp/Ollama on inference (we integrate with them)

## Design Principles

### 1. Lean Core, Extensible Edges

- Prefer a lean core with a simple agent / execution loop
- Push complexity to configuration and pluggable components
- Add features only when real use cases demand them
- Every dependency must justify its inclusion

### 2. Durable Artifacts, Minimal State

- Keep the core process stateless and restart-safe
- Default to file-based state; when state needs a service, make it explicit and exportable
- Support optional external backends for advanced deployments
- Agent definitions must be portable across runtimes

### 3. Observability-First

- Make behavior inspectable: tool calls, file reads/writes, commands, and errors
- Emit a structured event stream suitable for UIs and debugging
- Avoid "black boxes": clients should be able to replay and audit what happened

### 4. Context Management — Minimal, Sufficient, Intentional

- Keep prompts minimal and explicit
- Use progressive disclosure for tools/docs rather than dumping everything up front
- Prefer on-demand loading of files and context

### 5. Separate Content from Details

Tool results should separate:
- **Content**: What the LLM sees (text/JSON)
- **Details**: What the UI/client uses for rendering (structured metadata)

This prevents models from parsing textual output meant for interfaces.

### 6. Embedded-Compatible by Default

- Design decisions should not preclude running on constrained hardware
- Avoid bloat: no unnecessary dependencies, no heavy runtimes
- Prefer lightweight tool integrations (CLI tools over heavy MCP servers)

### 7. Practical Security, Context-Aware Controls

- Default to a permissive experience for trusted, single-user setups
- Provide optional sandboxing for untrusted or multi-tenant contexts
- Make capabilities explicit and configurable (what tools exist, what they can access)

## Tech Stack

| Component | Choice | Rationale |
|-----------|--------|-----------|
| **Language** | Rust | Fast, small binaries (<5MB), no runtime, excellent ARM support, memory-safe, single binary deployment |
| **HTTP** | `axum` | Async, performant, production-ready |
| **CLI** | `clap` | Standard Rust CLI library |
| **Config** | YAML + Markdown | YAML for structured config; Markdown for prompts/instructions |
| **Serialization** | `serde` | De facto Rust standard |
| **Default state store** | File-based (filesystem) | Zero dependencies, portable |
| **LLM Client** | Custom (reqwest) | No heavy SDK dependencies |
| **MCP Client** | Custom | Implement MCP protocol directly |

### Why Rust over Go?

| Factor | Rust | Go |
|--------|------|-----|
| **Binary size** | 2-5MB typical | 10-15MB typical |
| **Memory usage** | No GC, predictable | GC pauses, higher baseline |
| **ARM support** | Excellent, first-class | Good |
| **Embedded suitability** | Better (no runtime) | Good (small runtime) |
| **Safety** | Memory-safe, no null | GC-safe, has nil |
| **Ecosystem for LLM** | Growing (llama.cpp bindings) | Good (Ollama is Go) |
| **Learning curve** | Steeper | Gentler |

Rust's smaller binaries and lower memory footprint align better with the embedded secondary goal while maintaining performance for cloud deployments.

## Competitive Landscape

### Primary Competitors (Agent Runtimes)

| Project | Differentiator vs Agnx |
|---------|----------------------|
| **Agno/AgentOS** | Python/FastAPI, requires PostgreSQL, heavier footprint |
| **Dify** | Full platform with UI, requires Docker + PostgreSQL + Redis |
| **Microsoft Agent Framework** | Heavy SDK, Python/.NET, enterprise-focused |
| **LangGraph** | Python SDK, code-first not file-first |

### Why Agnx is Different

1. **Single binary** — No Python, no Docker Compose, no database required
2. **File-first** — Agent definitions are portable YAML/Markdown files
3. **Embedded-capable** — Runs on Raspberry Pi, edge devices, IoT gateways
4. **Standards-first** — Agent Protocol, MCP, A2A — not proprietary formats

### Market Positioning

```
                    Heavy
                      │
         Dify ────────┼──────── Microsoft Agent Framework
                      │
                      │
    LangGraph ────────┼──────── Agno
                      │
                      │
                      │
         ────────────┼──────────────────
                      │
                      │              Agnx
                      │         (Cloud + Edge)
                      │
                    Light

        Code-First ◄─────────────► File-First
```

## Standards Alignment

| Standard | How Agnx Aligns |
|----------|------------------|
| **[Agent Protocol](https://agentprotocol.ai/)** | Implements task/step management endpoints |
| **[A2A Protocol](https://a2a-protocol.org/)** | Agent Card for discovery |
| **[MCP](https://modelcontextprotocol.io/)** | Tools integrate via MCP protocol |
| **[AGENTS.md](https://agents.md/)** | Supports AGENTS.md for repository context |

## Roadmap

### v0.1.0 — Foundation

- [ ] Agent spec loader (AAF: YAML + Markdown)
- [ ] Single LLM provider (Ollama for local-first)
- [ ] Basic agent executor (prompt → response)
- [ ] HTTP API (minimal endpoints)
- [ ] CLI: `agnx serve`, `agnx chat`
- [ ] ARM64 + x86_64 builds
- [ ] Docker image

### v0.2.0 — Standards & Providers

- [ ] Agent Protocol API (`/api/v1/agent/tasks`)
- [ ] Multiple LLM providers (OpenAI, Anthropic, OpenRouter)
- [ ] Services interfaces (Session/Memory/Artifacts)
- [ ] CLI: `agnx run`, `agnx validate`
- [ ] Raspberry Pi testing in CI

### v0.3.0 — Tools & Memory

- [ ] MCP tool integration
- [ ] CLI tool support (lightweight alternative to MCP)
- [ ] File-based memory backend
- [ ] Agent export/import
- [ ] CLI: `agnx export`, `agnx import`

### v0.4.0 — Production Ready

- [ ] External storage backends (PostgreSQL, Redis, S3)
- [ ] Comprehensive test suite
- [ ] OpenAPI documentation
- [ ] Performance benchmarks (x86, ARM)
- [ ] Security audit
- [ ] `agnx-lite` minimal build

### v1.0.0 — Stable Release

- [ ] Stable API (no breaking changes)
- [ ] Full documentation
- [ ] Helm chart for Kubernetes
- [ ] Published to package managers (cargo, homebrew, apt)
- [ ] Community contributions welcome

## Success Metrics

1. **Adoption**: 1000+ GitHub stars, 100+ production deployments
2. **Performance**: <10ms startup, <50MB memory on Raspberry Pi
3. **Portability**: Agent definitions work across 3+ runtimes
4. **Standards**: Full Agent Protocol + MCP compliance
5. **Community**: 10+ contributors, active Discord/forum

## References

### Standards

- [Agent Protocol](https://agentprotocol.ai/)
- [A2A Protocol](https://a2a-protocol.org/)
- [Model Context Protocol](https://modelcontextprotocol.io/)
- [AGENTS.md](https://agents.md/)
- [Open Agent Specification](https://github.com/oracle/agent-spec)

### Inspiration

- [Ollama](https://ollama.com/) — Proved single-binary local LLM is viable
- [nginx](https://nginx.org/) — Stateless, config-driven, ubiquitous
- [Caddy](https://caddyserver.com/) — Modern single-binary web server in Go
- [llama.cpp](https://github.com/ggml-org/llama.cpp) — Lightweight LLM inference

### Similar Projects

- [Agno](https://github.com/agno-agi/agno) — Python agent runtime
- [Dify](https://github.com/langgenius/dify) — Full agent platform
- [mcp-agent](https://github.com/lastmile-ai/mcp-agent) — MCP-native agent framework
